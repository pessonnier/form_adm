{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaReceiveDataPy\n",
    "This notebook receives data from Kafka on the topic 'test', and stores it in the 'time_test' table of Cassandra (created by cassandra_init.script in startup_script.sh).\n",
    "\n",
    "```\n",
    "CREATE KEYSPACE test_time WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "CREATE TABLE test_time.sent_received(\n",
    " time_sent TEXT,\n",
    " time_received TEXT,\n",
    "PRIMARY KEY (time_sent)\n",
    ");\n",
    "```\n",
    "\n",
    "A message that gives the current time is received every second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and start SparkContext\n",
    "Note that SparkContext must be started to effectively load the package dependencies. Two cores are used, since one is needed for running the Kafka receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming test\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveToCassandra function\n",
    "Takes a list of tuple (rows) and save to Cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"sent_received\", keyspace=\"test_time\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create streaming task\n",
    "* Receive data from Kafka 'test' topic every five seconds\n",
    "* Get stream content, and add receiving time to each message\n",
    "* Save each RDD in the DStream to Cassandra. Also print on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5)\n",
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'test': 1})\n",
    "data = kvs.map(lambda x: x[1])\n",
    "rows= data.map(lambda x:Row(time_sent=x,time_received=time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "rows.foreachRDD(saveToCassandra)\n",
    "rows.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sqlContext.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"sent_received\", keyspace=\"test_time\")\\\n",
    "    .load()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.registerTempTable(\"sent_received\");\n",
    "data.printSchema()\n",
    "data=sqlContext.sql(\"select * from sent_received\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE KEYSPACE test_stocks WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "\n",
    "CREATE TABLE test_stocks.sent_received(time TEXT, exchange TEXT, date TEXT, price TEXT, PRIMARY KEY (time));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming test\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    print('boo')\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"sent_received\", keyspace=\"test_stocks\")\\\n",
    "        .save()\n",
    "    else:\n",
    "        print('rien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parserowrecue(x):\n",
    "    l=x.split(',')\n",
    "    return Row(time=l[0], exchange=l[1], date=l[2], price=l[3])\n",
    "ssc = StreamingContext(sc, 5)\n",
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'test_stock': 1})\n",
    "data = kvs.map(lambda x: x[1])\n",
    "rows= data.map(parserowrecue)\n",
    "rows.foreachRDD(saveToCassandra)\n",
    "rows.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:01:50\n",
      "-------------------------------------------\n",
      "Row(date=u'2009-03-23', exchange=u'NYSE', price=u'4.53', time=u'2019-04-25 12:52:34')\n",
      "Row(date=u'2009-03-20', exchange=u'NYSE', price=u'5.15', time=u'2019-04-25 12:52:35')\n",
      "Row(date=u'2009-03-19', exchange=u'NYSE', price=u'5.3', time=u'2019-04-25 12:52:36')\n",
      "Row(date=u'2009-03-18', exchange=u'NYSE', price=u'4.05', time=u'2019-04-25 12:52:37')\n",
      "Row(date=u'2009-03-17', exchange=u'NYSE', price=u'3.78', time=u'2019-04-25 12:52:38')\n",
      "Row(date=u'2009-03-16', exchange=u'NYSE', price=u'4.08', time=u'2019-04-25 12:52:39')\n",
      "Row(date=u'2009-03-13', exchange=u'NYSE', price=u'3.95', time=u'2019-04-25 12:52:40')\n",
      "Row(date=u'2009-03-12', exchange=u'NYSE', price=u'3.46', time=u'2019-04-25 12:52:41')\n",
      "Row(date=u'2009-03-11', exchange=u'NYSE', price=u'3.68', time=u'2019-04-25 12:52:42')\n",
      "Row(date=u'2009-03-10', exchange=u'NYSE', price=u'3.42', time=u'2019-04-25 12:52:43')\n",
      "...\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:01:55\n",
      "-------------------------------------------\n",
      "Row(date=u'2007-01-08', exchange=u'NYSE', price=u'70.96', time=u'2019-04-25 13:01:50')\n",
      "Row(date=u'2007-01-05', exchange=u'NYSE', price=u'71.29', time=u'2019-04-25 13:01:51')\n",
      "Row(date=u'2007-01-04', exchange=u'NYSE', price=u'71.78', time=u'2019-04-25 13:01:52')\n",
      "Row(date=u'2007-01-03', exchange=u'NYSE', price=u'72.22', time=u'2019-04-25 13:01:53')\n",
      "Row(date=u'2006-12-29', exchange=u'NYSE', price=u'72.5', time=u'2019-04-25 13:01:54')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:00\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-12-28', exchange=u'NYSE', price=u'72.24', time=u'2019-04-25 13:01:55')\n",
      "Row(date=u'2006-12-27', exchange=u'NYSE', price=u'71.83', time=u'2019-04-25 13:01:56')\n",
      "Row(date=u'2006-12-26', exchange=u'NYSE', price=u'71.05', time=u'2019-04-25 13:01:57')\n",
      "Row(date=u'2006-12-22', exchange=u'NYSE', price=u'71.14', time=u'2019-04-25 13:01:58')\n",
      "Row(date=u'2006-12-21', exchange=u'NYSE', price=u'71.47', time=u'2019-04-25 13:01:59')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:05\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-12-20', exchange=u'NYSE', price=u'71.7', time=u'2019-04-25 13:02:00')\n",
      "Row(date=u'2006-12-19', exchange=u'NYSE', price=u'72.01', time=u'2019-04-25 13:02:01')\n",
      "Row(date=u'2006-12-18', exchange=u'NYSE', price=u'72.07', time=u'2019-04-25 13:02:02')\n",
      "Row(date=u'2006-12-15', exchange=u'NYSE', price=u'71.34', time=u'2019-04-25 13:02:03')\n",
      "Row(date=u'2006-12-14', exchange=u'NYSE', price=u'70.79', time=u'2019-04-25 13:02:04')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:10\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-12-13', exchange=u'NYSE', price=u'71.65', time=u'2019-04-25 13:02:05')\n",
      "Row(date=u'2006-12-12', exchange=u'NYSE', price=u'71.0', time=u'2019-04-25 13:02:06')\n",
      "Row(date=u'2006-12-11', exchange=u'NYSE', price=u'70.88', time=u'2019-04-25 13:02:07')\n",
      "Row(date=u'2006-12-08', exchange=u'NYSE', price=u'71.01', time=u'2019-04-25 13:02:08')\n",
      "Row(date=u'2006-12-07', exchange=u'NYSE', price=u'70.96', time=u'2019-04-25 13:02:09')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:15\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-12-06', exchange=u'NYSE', price=u'71.05', time=u'2019-04-25 13:02:10')\n",
      "Row(date=u'2006-12-05', exchange=u'NYSE', price=u'70.72', time=u'2019-04-25 13:02:11')\n",
      "Row(date=u'2006-12-04', exchange=u'NYSE', price=u'70.55', time=u'2019-04-25 13:02:12')\n",
      "Row(date=u'2006-12-01', exchange=u'NYSE', price=u'71.13', time=u'2019-04-25 13:02:13')\n",
      "Row(date=u'2006-11-30', exchange=u'NYSE', price=u'71.05', time=u'2019-04-25 13:02:14')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:20\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-11-29', exchange=u'NYSE', price=u'70.62', time=u'2019-04-25 13:02:15')\n",
      "Row(date=u'2006-11-28', exchange=u'NYSE', price=u'70.89', time=u'2019-04-25 13:02:16')\n",
      "Row(date=u'2006-11-27', exchange=u'NYSE', price=u'71.51', time=u'2019-04-25 13:02:17')\n",
      "Row(date=u'2006-11-24', exchange=u'NYSE', price=u'71.89', time=u'2019-04-25 13:02:18')\n",
      "Row(date=u'2006-11-22', exchange=u'NYSE', price=u'72.35', time=u'2019-04-25 13:02:19')\n",
      "\n",
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:25\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-11-21', exchange=u'NYSE', price=u'72.33', time=u'2019-04-25 13:02:20')\n",
      "Row(date=u'2006-11-20', exchange=u'NYSE', price=u'71.76', time=u'2019-04-25 13:02:21')\n",
      "Row(date=u'2006-11-17', exchange=u'NYSE', price=u'71.98', time=u'2019-04-25 13:02:22')\n",
      "Row(date=u'2006-11-16', exchange=u'NYSE', price=u'71.58', time=u'2019-04-25 13:02:23')\n",
      "Row(date=u'2006-11-15', exchange=u'NYSE', price=u'70.94', time=u'2019-04-25 13:02:24')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boo\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:30\n",
      "-------------------------------------------\n",
      "Row(date=u'2006-11-14', exchange=u'NYSE', price=u'70.84', time=u'2019-04-25 13:02:25')\n",
      "Row(date=u'2006-11-13', exchange=u'NYSE', price=u'70.9', time=u'2019-04-25 13:02:26')\n",
      "Row(date=u'2006-11-10', exchange=u'NYSE', price=u'70.9', time=u'2019-04-25 13:02:27')\n",
      "Row(date=u'2006-11-09', exchange=u'NYSE', price=u'71.24', time=u'2019-04-25 13:02:28')\n",
      "\n",
      "boo\n",
      "rien\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:35\n",
      "-------------------------------------------\n",
      "\n",
      "boo\n",
      "rien\n",
      "-------------------------------------------\n",
      "Time: 2019-04-25 13:02:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blabla'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Row(date='2222-11-22', exchange='XXXX', price='123', time='blabla').time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StringType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-256170c2b8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2222-11-22'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'XXXX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'123'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blabla'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.sql.cassandra\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sent_received\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_stocks\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'StringType' is not defined"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(Row(date=StringType('2222-11-22'), exchange='XXXX', price='123', time='blabla')).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"sent_received\", keyspace=\"test_stocks\")\\\n",
    "        .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
